{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ğŸ”¥ VTrackAI Studio - E2E GPU Setup\n",
                "\n",
                "**Prerequisites:**\n",
                "1. Runtime â†’ Change runtime type â†’ **T4 GPU**\n",
                "2. Sidebar â†’ Secrets â†’ Add `HF_TOKEN` (from huggingface.co/settings/tokens)\n",
                "3. Run all cells (Ctrl+F9)\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 1: SETUP (Run once per session, ~3-5 min)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import torch, os, sys\n",
                "\n",
                "# GPU Check\n",
                "print(\"=\"*60)\n",
                "if torch.cuda.is_available():\n",
                "    print(f\"âœ… GPU: {torch.cuda.get_device_name(0)}\")\n",
                "    print(f\"ğŸ“Š VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
                "else:\n",
                "    print(\"âŒ NO GPU! Go to Runtime â†’ Change runtime type â†’ T4 GPU\")\n",
                "    sys.exit(1)\n",
                "print(\"=\"*60)\n",
                "\n",
                "# Clean start\n",
                "%cd /content\n",
                "!rm -rf /content/sam3 /content/vtrack-ai-studio\n",
                "\n",
                "# Install dependencies\n",
                "print(\"\\nğŸ“¦ Installing dependencies...\")\n",
                "!pip install -q torch torchvision --index-url https://download.pytorch.org/whl/cu121\n",
                "!pip install -q fastapi uvicorn[standard] python-multipart huggingface_hub requests decord\n",
                "!pip install -q numpy==1.26.4  # SAM3 requires numpy<2\n",
                "\n",
                "# Node.js 18 via nvm\n",
                "print(\"\\nğŸ“¦ Installing Node.js 18...\")\n",
                "!curl -so- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.7/install.sh | bash > /dev/null 2>&1\n",
                "import subprocess\n",
                "result = subprocess.run([\"bash\", \"-lc\", '''\n",
                "  export NVM_DIR=\"$HOME/.nvm\"\n",
                "  [ -s \"$NVM_DIR/nvm.sh\" ] && . \"$NVM_DIR/nvm.sh\"\n",
                "  nvm install 18 > /dev/null 2>&1\n",
                "  nvm use 18 > /dev/null 2>&1\n",
                "  echo \"Node $(node -v), npm $(npm -v)\"\n",
                "'''], capture_output=True, text=True)\n",
                "print(f\"âœ… {result.stdout.strip()}\")\n",
                "\n",
                "# Clone SAM3\n",
                "print(\"\\nğŸ“¦ Cloning SAM3...\")\n",
                "%cd /content\n",
                "!git clone -q https://github.com/facebookresearch/sam3\n",
                "!pip install -e /content/sam3 -q 2>/dev/null\n",
                "\n",
                "# Clone VTrackAI\n",
                "print(\"ğŸ“¦ Cloning VTrackAI...\")\n",
                "!git clone -q https://github.com/Amitro123/vtrack-ai-studio\n",
                "%cd /content/vtrack-ai-studio/backend\n",
                "!pip install -r requirements.txt -q 2>/dev/null\n",
                "\n",
                "# HuggingFace login + checkpoint\n",
                "print(\"\\nğŸ” Downloading SAM3 checkpoint...\")\n",
                "from google.colab import userdata\n",
                "from huggingface_hub import login, hf_hub_download\n",
                "\n",
                "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
                "login(token=HF_TOKEN, add_to_git_credential=False)\n",
                "\n",
                "os.makedirs(\"/content/vtrack-ai-studio/backend/checkpoints/sam3\", exist_ok=True)\n",
                "checkpoint_path = hf_hub_download(\n",
                "    repo_id=\"facebook/sam3\",\n",
                "    filename=\"sam3.pt\",\n",
                "    local_dir=\"/content/vtrack-ai-studio/backend/checkpoints/sam3\",\n",
                "    token=HF_TOKEN,\n",
                ")\n",
                "\n",
                "# Create symlinks\n",
                "sam3_pt = \"/content/vtrack-ai-studio/backend/checkpoints/sam3/sam3.pt\"\n",
                "for dst in [\n",
                "    \"/content/vtrack-ai-studio/backend/checkpoints/sam3/sam3_hiera_large.pt\",\n",
                "]:\n",
                "    if os.path.exists(dst): os.remove(dst)\n",
                "    os.symlink(sam3_pt, dst)\n",
                "\n",
                "print(f\"âœ… Checkpoint ready: {checkpoint_path}\")\n",
                "print(\"\\n\" + \"=\"*60)\n",
                "print(\"âœ… SETUP COMPLETE! Run Cell 2 to start backend.\")\n",
                "print(\"=\"*60)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 2: START BACKEND (FastAPI on port 8000)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import subprocess, time, requests, os\n",
                "\n",
                "# Kill any existing backend\n",
                "!pkill -9 -f uvicorn 2>/dev/null || true\n",
                "time.sleep(2)\n",
                "\n",
                "# Start backend in background\n",
                "print(\"ğŸš€ Starting FastAPI backend...\")\n",
                "log_file = open(\"/content/backend.log\", \"w\")\n",
                "proc = subprocess.Popen(\n",
                "    [\"uvicorn\", \"server:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"],\n",
                "    stdout=log_file,\n",
                "    stderr=subprocess.STDOUT,\n",
                "    cwd=\"/content/vtrack-ai-studio/backend\",\n",
                ")\n",
                "\n",
                "# Wait for backend with retries\n",
                "for i in range(20):\n",
                "    if proc.poll() is not None:\n",
                "        print(f\"âŒ Backend crashed! Logs:\")\n",
                "        !cat /content/backend.log\n",
                "        break\n",
                "    try:\n",
                "        r = requests.get(\"http://localhost:8000/api/health\", timeout=3)\n",
                "        health = r.json()\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(f\"âœ… Backend running on port 8000\")\n",
                "        print(f\"   SAM3: {'âœ… Available' if health.get('sam3_available') else 'âŒ Not loaded'}\")\n",
                "        print(f\"   Device: {health.get('device', 'unknown')}\")\n",
                "        print(\"=\"*60)\n",
                "        break\n",
                "    except:\n",
                "        print(f\"â³ Waiting... ({i+1}/20)\")\n",
                "        time.sleep(3)\n",
                "else:\n",
                "    print(\"âŒ Backend failed to start. Logs:\")\n",
                "    !cat /content/backend.log\n",
                "\n",
                "print(\"\\nâœ… Run Cell 3 to start frontend.\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 3: START FRONTEND (Vite + Cloudflare tunnel)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "import subprocess, time\n",
                "\n",
                "# Start Vite frontend\n",
                "print(\"ğŸš€ Starting Vite frontend...\")\n",
                "bash_cmd = '''\n",
                "cd /content/vtrack-ai-studio\n",
                "export NVM_DIR=\"$HOME/.nvm\"\n",
                "[ -s \"$NVM_DIR/nvm.sh\" ] && . \"$NVM_DIR/nvm.sh\"\n",
                "nvm use 18 > /dev/null 2>&1\n",
                "echo 'VITE_API_URL=http://localhost:8000' > .env\n",
                "npm install --silent 2>/dev/null\n",
                "nohup npm run dev -- --host 0.0.0.0 --port 4173 > /content/vite.log 2>&1 &\n",
                "echo \"Vite PID: $!\"\n",
                "'''\n",
                "result = subprocess.run([\"bash\", \"-lc\", bash_cmd], capture_output=True, text=True)\n",
                "print(result.stdout)\n",
                "\n",
                "# Wait for Vite to start\n",
                "time.sleep(10)\n",
                "\n",
                "# Start Cloudflare tunnel\n",
                "print(\"ğŸŒ Creating Cloudflare tunnel...\")\n",
                "!pip install -q cloudflared 2>/dev/null\n",
                "!pkill cloudflared 2>/dev/null || true\n",
                "!nohup cloudflared tunnel --url http://localhost:4173 > /content/tunnel.log 2>&1 &\n",
                "\n",
                "# Wait and extract URL\n",
                "time.sleep(8)\n",
                "import re\n",
                "try:\n",
                "    with open(\"/content/tunnel.log\") as f:\n",
                "        log = f.read()\n",
                "    urls = re.findall(r'https://[\\w-]+\\.trycloudflare\\.com', log)\n",
                "    if urls:\n",
                "        print(\"\\n\" + \"=\"*60)\n",
                "        print(\"ğŸ‰ VTrackAI Studio is LIVE!\")\n",
                "        print(f\"\\n   ğŸ‘‰ {urls[0]}\")\n",
                "        print(\"\\n   Open this URL in your browser.\")\n",
                "        print(\"=\"*60)\n",
                "    else:\n",
                "        print(\"â³ Waiting for tunnel URL...\")\n",
                "        !cat /content/tunnel.log | grep -o 'https://.*trycloudflare.com' | head -1\n",
                "except:\n",
                "    !cat /content/tunnel.log | grep -o 'https://.*trycloudflare.com' | head -1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "# CELL 4: DEBUG / LOGS (Run if needed)\n",
                "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
                "print(\"=== Backend Logs ===\")\n",
                "!tail -20 /content/backend.log 2>/dev/null || echo \"No backend log\"\n",
                "\n",
                "print(\"\\n=== Vite Logs ===\")\n",
                "!tail -10 /content/vite.log 2>/dev/null || echo \"No vite log\"\n",
                "\n",
                "print(\"\\n=== Tunnel Logs ===\")\n",
                "!tail -10 /content/tunnel.log 2>/dev/null || echo \"No tunnel log\"\n",
                "\n",
                "print(\"\\n=== Process Status ===\")\n",
                "!ps aux | grep -E 'uvicorn|node|cloudflared' | grep -v grep"
            ]
        }
    ],
    "metadata": {
        "accelerator": "GPU",
        "colab": {
            "gpuType": "T4",
            "provenance": []
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}