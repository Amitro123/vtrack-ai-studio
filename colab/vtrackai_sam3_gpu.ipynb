{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "title"
      },
      "source": [
        "# ğŸš€ VTrackAI Studio â€“ Full Colab GPU (SAM3 + Frontend)\n",
        "\n",
        "**Backend + Frontend ×¨×¦×™× ×‘×ª×•×š Colab ×¢×œ T4 GPU.**\n",
        "\n",
        "**Flow:**\n",
        "1. Runtime â†’ **T4 GPU**\n",
        "2. Sidebar â†’ **Secrets** â†’ Add `HF_TOKEN`\n",
        "3. Runtime â†’ **Restart session**\n",
        "4. Ctrl+F9 (**Run all**) â€“ ×•×‘×¡×•×£ ×ª×§×‘×œ ×§×™×©×•×¨×™× ×œ×¤×ª×™×—×ª ×”â€‘UI ×•×”â€‘API ×‘×“×¤×“×¤×Ÿ.\n",
        "\n",
        "**Important:** ××™×Ÿ ×©×™××•×© ×‘â€‘ngrok / cloudflare / localtunnel. Colab ×™×™×ª×Ÿ ×œ×š URLs ×œâ€‘ports."
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gpu-check"
      },
      "source": [
        "## 1. GPU Check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gpu-check-code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"ğŸ”¥ CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"ğŸ® GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"ğŸ“Š VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"âš ï¸ Set runtime type to GPU (T4) and rerun.\")"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deps"
      },
      "source": [
        "## 2. Install Backend + Frontend Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deps-code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "!rm -rf /content/sam3 /content/vtrack-ai-studio\n",
        "\n",
        "# PyTorch + core libs (GPU)\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q fastapi uvicorn[standard] python-multipart opencv-python librosa huggingface_hub nest-asyncio\n",
        "\n",
        "# Node / frontend deps\n",
        "!apt-get -y install nodejs npm > /dev/null 2>&1\n",
        "\n",
        "print(\"âœ… Dependencies installed\")"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clone"
      },
      "source": [
        "## 3. Clone SAM3 + VTrackAI and Install"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clone-code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone https://github.com/facebookresearch/sam3\n",
        "!pip install -e ./sam3 --quiet\n",
        "\n",
        "!git clone https://github.com/Amitro123/vtrack-ai-studio\n",
        "%cd /content/vtrack-ai-studio/backend\n",
        "!pip install -r requirements.txt --quiet\n",
        "\n",
        "print(\"âœ… SAM3 + VTrackAI backend ready\")"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hf-login"
      },
      "source": [
        "## 4. HF Login from Secrets + Download SAM3 Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf-login-code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "from huggingface_hub import login, hf_hub_download\n",
        "import os\n",
        "\n",
        "HF_TOKEN = userdata.get('HF_TOKEN')\n",
        "login(token=HF_TOKEN)\n",
        "print('âœ… HF logged in from Secrets')\n",
        "\n",
        "os.makedirs(\"/content/vtrack-ai-studio/backend/checkpoints/sam3\", exist_ok=True)\n",
        "%cd /content/vtrack-ai-studio/backend\n",
        "\n",
        "checkpoint_path = hf_hub_download(\n",
        "    repo_id=\"facebook/sam3\",\n",
        "    filename=\"sam3.pt\",\n",
        "    local_dir=\"./checkpoints/sam3\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "print(f\"âœ… SAM3 checkpoint: {checkpoint_path}\")\n",
        "print(f\"ğŸ“ Size: {os.path.getsize(checkpoint_path)/1e9:.1f} GB\")\n",
        "\n",
        "sam3_pt = \"./checkpoints/sam3/sam3.pt\"\n",
        "try:\n",
        "    os.symlink(sam3_pt, \"./checkpoints/sam3/sam3_hiera_large.pt\")\n",
        "except FileExistsError:\n",
        "    pass\n",
        "os.makedirs(\"./checkpoints/sam3/checkpoints\", exist_ok=True)\n",
        "try:\n",
        "    os.symlink(sam3_pt, \"./checkpoints/sam3/checkpoints/sam3_hiera_tiny.pt\")\n",
        "except FileExistsError:\n",
        "    pass\n",
        "print(\"âœ… Symlinks ready for backend\")"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {
        "id": "backend-start"
      },
      "source": [
        "## 5. Start FastAPI Backend on GPU (Port 8000)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "backend-start-code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "import os, time, multiprocessing as mp, requests, nest_asyncio\n",
        "import uvicorn\n",
        "\n",
        "os.chdir('/content/vtrack-ai-studio/backend')\n",
        "print(f\"âœ… Backend dir: {os.getcwd()}\")\n",
        "\n",
        "!pkill -f uvicorn || true\n",
        "time.sleep(2)\n",
        "nest_asyncio.apply()\n",
        "\n",
        "def run_uvicorn():\n",
        "    uvicorn.run(\"server:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "p = mp.Process(target=run_uvicorn)\n",
        "p.start()\n",
        "print(f\"ğŸš€ Uvicorn started with PID: {p.pid}\")\n",
        "time.sleep(8)\n",
        "\n",
        "try:\n",
        "    r = requests.get(\"http://127.0.0.1:8000/api/health\", timeout=10)\n",
        "    print(\"âœ… Health:\", r.json())\n",
        "except Exception as e:\n",
        "    print(\"âŒ Health failed:\", e)"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frontend-env"
      },
      "source": [
        "## 6. Configure Frontend to Talk to Local Backend"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frontend-env-code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%cd /content/vtrack-ai-studio\n",
        "with open('.env', 'w') as f:\n",
        "    f.write('VITE_API_URL=http://localhost:8000\\n')\n",
        "print('âœ… .env updated with VITE_API_URL=http://localhost:8000')"
      ]
    },

    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frontend-start"
      },
      "source": [
        "## 7. Start Frontend (Vite) â€“ Port 4173\n",
        "\n",
        "×”×¨×¥ ×ª× ×–×” **×‘×¡×•×£** ×•×ª×©××™×¨ ××•×ª×• ×¤×ª×•×—. Colab ×™×¦×™×’ ×§×™×©×•×¨ ×œ×¤×ª×™×—×ª ×”â€‘UI ×‘×“×¤×“×¤×Ÿ."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frontend-start-code"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "%cd /content/vtrack-ai-studio\n",
        "!npm install --silent\n",
        "\n",
        "print('\\nğŸš€ Starting Vite dev server on port 4173 (host=0.0.0.0)...')\n",
        "print('â„¹ï¸ Colab usually shows a link like: https://xxxx-4173.colab.googleusercontent.com')\n",
        "print('   Open that URL in your browser to use VTrackAI Studio UI.\\n')\n",
        "\n",
        "!npm run dev -- --host 0.0.0.0 --port 4173"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
