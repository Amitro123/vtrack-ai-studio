{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SP066JgO7Onb"
      },
      "source": [
        "# üöÄ VTrackAI Studio - GPU Backend (SAM3)\n",
        "\n",
        "**Connect local frontend to Colab GPU for 10x faster SAM3 tracking!**\n",
        "\n",
        "**Requirements:**\n",
        "- Colab GPU runtime (T4/A100)\n",
        "- Hugging Face account (free)\n",
        "\n",
        "**Steps:**\n",
        "1. Runtime ‚Üí Change runtime type ‚Üí T4 GPU\n",
        "2. Run all cells (Ctrl+F9)\n",
        "3. Copy **Cloudflare URL** (https://xxx.trycloudflare.com)\n",
        "4. Local: `VITE_API_URL=<url> npm run dev`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mOA7HdC7Ond"
      },
      "source": [
        "## 1. Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bRgU7QMr7One"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "print(f\"üî• CUDA Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"üìä VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No GPU - Runtime > Change runtime type > GPU\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMQhgf8c7Onf"
      },
      "source": [
        "## 2. Clean Install + Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6aCEOAH7Ong"
      },
      "outputs": [],
      "source": [
        "!rm -rf /content/sam3 /content/vtrack-ai-studio\n",
        "!pip install -q torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
        "!pip install -q fastapi uvicorn[standard] python-multipart opencv-python librosa huggingface_hub\n",
        "!pip install -q cloudflared nest-asyncio\n",
        "print(\"‚úÖ All dependencies ready\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Je8VyMtO7Ong"
      },
      "source": [
        "## 3. Install SAM3 + VTrackAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTaM_LdF7Onh"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/facebookresearch/sam3\n",
        "!pip install -e ./sam3 --quiet\n",
        "!git clone https://github.com/Amitro123/vtrack-ai-studio\n",
        "%cd vtrack-ai-studio/backend\n",
        "!pip install -r requirements.txt --quiet\n",
        "print(\"‚úÖ SAM3 + VTrackAI installed\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iP0ue4xz7Oni"
      },
      "source": [
        "## 4. Hugging Face Login + SAM3 Checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lE5SpA9b7UgL"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsgFhZzY7Onj"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import hf_hub_download\n",
        "import os\n",
        "\n",
        "os.makedirs(\"checkpoints/sam3\", exist_ok=True)\n",
        "checkpoint_path = hf_hub_download(\n",
        "    repo_id=\"facebook/sam3-hiera-large\",\n",
        "    filename=\"sam3_hiera_large.pt\",\n",
        "    local_dir=\"./checkpoints/sam3\"\n",
        ")\n",
        "print(f\"‚úÖ Checkpoint: {checkpoint_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbqUlUah7Onk"
      },
      "source": [
        "## 5. üöÄ Launch GPU Backend (Cloudflare Tunnel)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EFqUBVx47Onl"
      },
      "outputs": [],
      "source": [
        "import nest_asyncio\n",
        "import uvicorn\n",
        "import subprocess\n",
        "import threading\n",
        "import time\n",
        "import requests\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Start FastAPI backend\n",
        "def start_backend():\n",
        "    uvicorn.run(\"server:app\", host=\"0.0.0.0\", port=8000, log_level=\"info\")\n",
        "\n",
        "backend_thread = threading.Thread(target=start_backend, daemon=True)\n",
        "backend_thread.start()\n",
        "time.sleep(8)  # Wait for startup\n",
        "\n",
        "print(\"üöÄ Backend started on http://localhost:8000\")\n",
        "print(\"‚è≥ Starting Cloudflare tunnel...\")\n",
        "\n",
        "# Cloudflare tunnel (FREE + stable)\n",
        "!nohup cloudflared tunnel --url http://localhost:8000 > cloudflared.log 2>&1 &\n",
        "time.sleep(8)\n",
        "\n",
        "# Get Cloudflare URL from logs\n",
        "!grep -o 'https://[^ ]*\\.trycloudflare\\.com' cloudflared.log || echo \"üîç Check cloudflared.log for URL\"\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üéâ VTrackAI GPU Backend LIVE!\")\n",
        "print(\"üì° Look for Cloudflare URL above (https://xxx.trycloudflare.com)\")\n",
        "print(\"=\"*60)\n",
        "print(\"üëâ Local frontend: VITE_API_URL=<cloudflare-url> npm run dev\")\n",
        "\n",
        "# Keep alive\n",
        "try:\n",
        "    while True:\n",
        "        time.sleep(60)\n",
        "except KeyboardInterrupt:\n",
        "    print(\"\\nüëã Backend stopped\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "health-test"
      },
      "source": [
        "## üîç Health Check (Run after backend starts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "health-check"
      },
      "outputs": [],
      "source": [
        "# Test backend health\n",
        "import requests\n",
        "r = requests.get(\"http://localhost:8000/api/health\")\n",
        "print(\"Health:\", r.json())\n",
        "print(\"‚úÖ GPU tracking ready! üöÄ\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
